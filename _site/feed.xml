<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-04-25T10:26:57+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">GGX</title><subtitle>Unofficial website for the Grenoble Graphics Research Team.</subtitle><entry><title type="html">A Dummy Publication</title><link href="http://localhost:4000/publication/2023/04/18/dummy.html" rel="alternate" type="text/html" title="A Dummy Publication" /><published>2023-04-18T00:00:00+02:00</published><updated>2023-04-18T00:00:00+02:00</updated><id>http://localhost:4000/publication/2023/04/18/dummy</id><content type="html" xml:base="http://localhost:4000/publication/2023/04/18/dummy.html">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Here put the paper’s abstract.&lt;/p&gt;</content><author><name>Laurent Belcour</name></author><category term="publication" /><summary type="html">Abstract Here put the paper’s abstract.</summary></entry><entry><title type="html">A Data-Driven Paradigm for Precomputed Radiance Transfer</title><link href="http://localhost:4000/publication/2022/07/12/publication-rendering-data-driven-prt.html" rel="alternate" type="text/html" title="A Data-Driven Paradigm for Precomputed Radiance Transfer" /><published>2022-07-12T00:00:00+02:00</published><updated>2022-07-12T00:00:00+02:00</updated><id>http://localhost:4000/publication/2022/07/12/publication-rendering-data-driven-prt</id><content type="html" xml:base="http://localhost:4000/publication/2022/07/12/publication-rendering-data-driven-prt.html">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;In this work, we explore a change of paradigm to build &lt;em&gt;Precomputed Radiance Transfer&lt;/em&gt; (PRT) methods in a data-driven way. This paradigm shift allows us to alleviate the difficulties of building traditional PRT methods such as defining a reconstruction basis, coding a dedicated path tracer to compute a transfer function, etc. Our objective is to pave the way for Machine Learned methods by providing a simple baseline algorithm. More specifically, we demonstrate real-time rendering of indirect illumination in hair and surfaces from a few measurements of direct lighting. We build our baseline from pairs of direct and indirect illumination renderings using only standard tools such as Singular Value Decomposition (SVD) to extract both the reconstruction basis and transfer function.&lt;/p&gt;</content><author><name>Laurent Belcour</name></author><category term="publication" /><category term="published" /><summary type="html">Abstract In this work, we explore a change of paradigm to build Precomputed Radiance Transfer (PRT) methods in a data-driven way. This paradigm shift allows us to alleviate the difficulties of building traditional PRT methods such as defining a reconstruction basis, coding a dedicated path tracer to compute a transfer function, etc. Our objective is to pave the way for Machine Learned methods by providing a simple baseline algorithm. More specifically, we demonstrate real-time rendering of indirect illumination in hair and surfaces from a few measurements of direct lighting. We build our baseline from pairs of direct and indirect illumination renderings using only standard tools such as Singular Value Decomposition (SVD) to extract both the reconstruction basis and transfer function.</summary></entry><entry><title type="html">Htex: Per-Halfedge Texturing for Abritrary Mesh Topologies</title><link href="http://localhost:4000/publication/2022/07/12/publication-texture_htex.html" rel="alternate" type="text/html" title="Htex: Per-Halfedge Texturing for Abritrary Mesh Topologies" /><published>2022-07-12T00:00:00+02:00</published><updated>2022-07-12T00:00:00+02:00</updated><id>http://localhost:4000/publication/2022/07/12/publication-texture_htex</id><content type="html" xml:base="http://localhost:4000/publication/2022/07/12/publication-texture_htex.html">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;We introduce per-halfedge texturing (Htex) a GPU-friendly method for texturing arbitrary polygon-meshes without an explicit parameterization. Htex builds upon the insight that halfedges encode an intrinsic triangulation for polygon meshes, where each halfedge spans a unique triangle with direct adjacency information. Rather than storing a separate texture per face of the input mesh as is done by previous parameterization-free texturing methods, Htex stores a square texture for each halfedge and its twin. We show that this simple change from face to halfedge induces two important properties for high performance parameterization-free texturing. First, Htex natively supports arbitrary polygons without requiring dedicated code for, e.g, non-quad faces. Second, Htex leads to a straightforward and efficient GPU implementation that uses only three texture-fetches per halfedge to produce continuous texturing across the entire mesh. We demonstrate the effectiveness of Htex by rendering production assets in real time.&lt;/p&gt;</content><author><name>Wilhem Barbier</name></author><category term="publication" /><category term="published" /><summary type="html">Abstract We introduce per-halfedge texturing (Htex) a GPU-friendly method for texturing arbitrary polygon-meshes without an explicit parameterization. Htex builds upon the insight that halfedges encode an intrinsic triangulation for polygon meshes, where each halfedge spans a unique triangle with direct adjacency information. Rather than storing a separate texture per face of the input mesh as is done by previous parameterization-free texturing methods, Htex stores a square texture for each halfedge and its twin. We show that this simple change from face to halfedge induces two important properties for high performance parameterization-free texturing. First, Htex natively supports arbitrary polygons without requiring dedicated code for, e.g, non-quad faces. Second, Htex leads to a straightforward and efficient GPU implementation that uses only three texture-fetches per halfedge to produce continuous texturing across the entire mesh. We demonstrate the effectiveness of Htex by rendering production assets in real time.</summary></entry><entry><title type="html">Bringing Linearly Transformed Cosines to Anisotropic GGX</title><link href="http://localhost:4000/publication/2022/04/13/brdf-ltc-aniso.html" rel="alternate" type="text/html" title="Bringing Linearly Transformed Cosines to Anisotropic GGX" /><published>2022-04-13T00:00:00+02:00</published><updated>2022-04-13T00:00:00+02:00</updated><id>http://localhost:4000/publication/2022/04/13/brdf-ltc-aniso</id><content type="html" xml:base="http://localhost:4000/publication/2022/04/13/brdf-ltc-aniso.html"></content><author><name>Aakash KT</name></author><category term="publication" /><category term="published" /><summary type="html"></summary></entry><entry><title type="html">Rendering Layered Materials with Diffuse Interfaces</title><link href="http://localhost:4000/publication/2022/04/13/publication-brdf-layered-diffuse.html" rel="alternate" type="text/html" title="Rendering Layered Materials with Diffuse Interfaces" /><published>2022-04-13T00:00:00+02:00</published><updated>2022-04-13T00:00:00+02:00</updated><id>http://localhost:4000/publication/2022/04/13/publication-brdf-layered-diffuse</id><content type="html" xml:base="http://localhost:4000/publication/2022/04/13/publication-brdf-layered-diffuse.html"></content><author><name>Heloise de Dinechin</name></author><category term="publication" /><category term="published" /><summary type="html"></summary></entry><entry><title type="html">A Halfedge Refinement Rule for Parallel Loop Subdivision</title><link href="http://localhost:4000/publication/2022/04/13/pubication-loop.html" rel="alternate" type="text/html" title="A Halfedge Refinement Rule for Parallel Loop Subdivision" /><published>2022-04-13T00:00:00+02:00</published><updated>2022-04-13T00:00:00+02:00</updated><id>http://localhost:4000/publication/2022/04/13/pubication-loop</id><content type="html" xml:base="http://localhost:4000/publication/2022/04/13/pubication-loop.html"></content><author><name>Kenneth Vanhoey</name></author><category term="publication" /><category term="published" /><summary type="html"></summary></entry><entry><title type="html">Visually-Uniform Reparametrization of Material Appearance through Density Redistribution</title><link href="http://localhost:4000/publication/2021/10/01/techreport-brdf_linear-param.html" rel="alternate" type="text/html" title="Visually-Uniform Reparametrization of Material Appearance through Density Redistribution" /><published>2021-10-01T00:00:00+02:00</published><updated>2021-10-01T00:00:00+02:00</updated><id>http://localhost:4000/publication/2021/10/01/techreport-brdf_linear-param</id><content type="html" xml:base="http://localhost:4000/publication/2021/10/01/techreport-brdf_linear-param.html">&lt;p&gt;In this work, we tackle the question of how to parameterize a material model (or more generically a model) such that changing one of its parameters will change the output &lt;em&gt;linearly&lt;/em&gt; for the viewer. If it is possible to measure the amount of visual change for any small change of the parameter, then we show that building a linear parameterization is akin to inverting the cumulative difference function (integral of visual change over the range of the parameter).&lt;/p&gt;

&lt;center&gt;
&lt;img width=&quot;80%&quot; src=&quot;/images/projects/brdf_linear_space/framework.png&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;With this framework and with a proper visual change function, one can linearize any material model’s parameters. For example, we provide three examples of reparameterization for roughness, edge-tint and sheen.&lt;/p&gt;

&lt;center&gt;
&lt;img width=&quot;60%&quot; src=&quot;/images/projects/brdf_linear_space/result.png&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;But we also show that visual change functions are hard to find. For example, even perceptual metrics that are learned from human subjects do not produce a consistent parameterization depending on the lighting conditions. While this is expected as human perception depends on the frequency content of the illumination, it makes it even harded to build generic linear parameterizations that are based on human vision (if not impossible).&lt;/p&gt;</content><author><name>Pascal Barla</name></author><category term="publication" /><category term="techreport" /><summary type="html">In this work, we tackle the question of how to parameterize a material model (or more generically a model) such that changing one of its parameters will change the output linearly for the viewer. If it is possible to measure the amount of visual change for any small change of the parameter, then we show that building a linear parameterization is akin to inverting the cumulative difference function (integral of visual change over the range of the parameter).</summary></entry><entry><title type="html">Experimenting With Concurrent Binary Trees for Large-scale Terrain Rendering</title><link href="http://localhost:4000/publication/2021/08/10/publication-siggraph-course-cbt.html" rel="alternate" type="text/html" title="Experimenting With Concurrent Binary Trees for Large-scale Terrain Rendering" /><published>2021-08-10T00:00:00+02:00</published><updated>2021-08-10T00:00:00+02:00</updated><id>http://localhost:4000/publication/2021/08/10/publication-siggraph-course-cbt</id><content type="html" xml:base="http://localhost:4000/publication/2021/08/10/publication-siggraph-course-cbt.html">&lt;!-- With the `url_outside` tag, I can reference an outside blog / website --&gt;</content><author><name>Thomas Deliot</name></author><category term="publication" /><category term="published" /><summary type="html"></summary></entry><entry><title type="html">Passing Multi-Channel Material Textures to a 3-Channel Loss</title><link href="http://localhost:4000/publication/2021/06/24/publication-3channels_loss.html" rel="alternate" type="text/html" title="Passing Multi-Channel Material Textures to a 3-Channel Loss" /><published>2021-06-24T00:00:00+02:00</published><updated>2021-06-24T00:00:00+02:00</updated><id>http://localhost:4000/publication/2021/06/24/publication-3channels_loss</id><content type="html" xml:base="http://localhost:4000/publication/2021/06/24/publication-3channels_loss.html">&lt;!-- With the `url_outside` tag, I can reference an outside blog / website --&gt;</content><author><name>Thomas Chambon</name></author><category term="publication" /><category term="published" /><summary type="html"></summary></entry><entry><title type="html">Lessons Learned and Improvements when Building Screen-Space Samplers with Blue-Noise Error Distribution</title><link href="http://localhost:4000/publication/2021/06/24/publication-sampling_bluenoise_sig21.html" rel="alternate" type="text/html" title="Lessons Learned and Improvements when Building Screen-Space Samplers with Blue-Noise Error Distribution" /><published>2021-06-24T00:00:00+02:00</published><updated>2021-06-24T00:00:00+02:00</updated><id>http://localhost:4000/publication/2021/06/24/publication-sampling_bluenoise_sig21</id><content type="html" xml:base="http://localhost:4000/publication/2021/06/24/publication-sampling_bluenoise_sig21.html">&lt;!-- With the `url_outside` tag, I can reference an outside blog / website --&gt;</content><author><name>Laurent Belcour</name></author><category term="publication" /><category term="published" /><summary type="html"></summary></entry></feed>